{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NFT Value Estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTNl49GE1UFm6VjuFtmBvV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrMazagngy/NFT-Value-Estimation-/blob/main/NFT_Value_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYxFnpKvqNLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27491708-51fc-4582-d5a4-941b7c57603b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eSniCc0LUV-"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import glob\n",
        "import math\n",
        "#Model Architcture\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Input, Flatten , Dense ,LSTM ,Dropout\n",
        "from keras.models import Model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts9X1yM6GdsN"
      },
      "source": [
        "'''\n",
        "Cryptopunk Collection Has 10,000 NFTs , so we loop through all of them\n",
        "And Get Only Those With Previous Sales\n",
        "'''\n",
        "for i in range (1,10000):\n",
        "  \n",
        "  url = \"https://api.opensea.io/api/v1/asset/0xb47e3cd837ddf8e4c57f05d70ab865de6e193bbb/\"+ str(i)\n",
        "  response = requests.request(\"GET\", url).json()\n",
        "\n",
        "  if response['last_sale'] != None :\n",
        "\n",
        "    print (i)\n",
        "    price = int(response['last_sale']['total_price']) / 1000000000000000000   #Prices Have 18 Decimals \n",
        "    with open('/content/drive/My Drive/NFT Value Estimation/price.txt', 'a+') as f: #create a text file if not existed with name (price.txt)\n",
        "      f.write(str(price)+'\\n')\n",
        "\n",
        "    img = Image.open(urlopen(response['image_url']))\n",
        "    img = np.array(img)\n",
        "    img_name = '/content/drive/My Drive/NFT Value Estimation/pic_'+str(i) # save images with this format {pic_number}\n",
        "    cv2.imwrite(img_name +'.png' , img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFIYrL1Ok4YX"
      },
      "source": [
        "#Prices for all extracted NFTs\n",
        "Y = np.loadtxt('/content/drive/My Drive/NFT Value Estimation/price.txt')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jBA8Ey6u-Y6"
      },
      "source": [
        "#Retrieve all images from its folder and store them in a list\n",
        "X = []\n",
        "\n",
        "images = glob.glob('/content/drive/My Drive/NFT Value Estimation/*.png')\n",
        "for  image in images :\n",
        "  X.append(image)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APR-L48ue7Tz"
      },
      "source": [
        "#Splitting Data into Tain / Validation / Test Sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.3, random_state = 1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMBZQzvmZ1mX"
      },
      "source": [
        "'''\n",
        "Custom Data Generator class to preprocess images to ResNet50 shape of (224,224) and normalize its values\n",
        "\n",
        "'''\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.x) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
        "        self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
        "        self.batch_size]\n",
        "\n",
        "        return np.array([\n",
        "            cv2.resize(cv2.imread(file_name), (224, 224))\n",
        "               for file_name in batch_x]), np.array(batch_y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3jA2z4fvVT_"
      },
      "source": [
        "Batch_size = 32\n",
        "\n",
        "train = CustomDataGen(X_train,y_train,Batch_size)\n",
        "validation = CustomDataGen(X_val,y_val,Batch_size)\n",
        "test = CustomDataGen(X_test,y_test,Batch_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyoOoQX6yG9Q"
      },
      "source": [
        "#Model Architcture \n",
        "input = Input(shape=(224, 224, 3))\n",
        "\n",
        "base = tf.keras.applications.ResNet50(include_top=False,weights=\"imagenet\" )\n",
        "base.trainable=False\n",
        "x = Flatten()(base(input))\n",
        "x = tf.expand_dims(x, -1)\n",
        "x = Dropout(0.3)(x)\n",
        "x = LSTM(50)(x)\n",
        "x = Dense(10)(x)\n",
        "x = Dense(1)(x)\n",
        "\n",
        "model = Model(inputs=input, outputs=x)\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL33agvivFQp"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y63OrsG7vC2V"
      },
      "source": [
        "Epochs = 10\n",
        "\n",
        "# define the checkpoint\n",
        "filepath = \"'/content/drive/My Drive/NFT Value Estimation/models.h5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(train,epochs=Epochs, validation_data = validation,callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}